---
title: "08  A Rapid Introduction to th R Language - Bioinformatics Data Skills - O'Reilly"
author: "Charles Drummer IV"
date: "3/31/2022"
output: html_document
---

“Part III. Practice: Bioinformatics Data Skills”

Excerpt From
Bioinformatics Data Skills
Vince Buffalo
https://books.apple.com/us/book/bioinformatics-data-skills/id1025338879
This material may be protected by copyright.

“Chapter 8. A Rapid Introduction to the R Language”

Excerpt From
Bioinformatics Data Skills
Vince Buffalo
https://books.apple.com/us/book/bioinformatics-data-skills/id1025338879
This material may be protected by copyright.

R language is a data programming language yo ucan use to explore and  understand data in an  open-ended, highly interactive, iterative way

R language allows for *exploratory data analysis (EDA)* 
- EDA is an approach emphasizing understanding data and data limitations though interactive investigation rather tha n explicit statistical modeling

EDA can  reveal patterns in bioinformatics data overlooked with statistical modeling or hypothesis testing approaches

Chapter goal: teach EDA skills to give the freedom to explore and experiment with your data at any stage of analysis

# Getting started with R and RStudio
RStudio has a "Console" and a "Terminal" shell.  While R can be run in the Console, installing R in the terminal first allows for running R code in-line in the coding window
```{installing R in terminal}
$ brew install r
```
Developing code in R is back-and-forth between
- writing code in a re-runnable script
- exploring data interactively in the R interpreter

The comprehensive R archive network (CRAN) is a collection of packages that extend R's functionality
- packages can be installed directly within R with 'install.packages()' function
```{installing ggplot2 with install.packages function in R}
# 
> install.packages("ggplot2")
```
# R language basics

## Simple calculations in R, calling functions and getting help in R
expression - a calculation or command enter into an R console
evaluated expression - a result printed to console after pressing 'enter'

Arithmetic operations follow basic order of operations; parenthesis often needed to indicate order of operations 
```{white space and order of operations}
> 3 + 4/2
[1] 5
> (3 + 4)/2
[1] 3.5
```
Functions can be used to perform mathematics operations
- functions take zero or more arguments, perform calculations and output a return value 
- R functions copy arguments, do not modify arguments in place 

Common mathematics functions in  R
- exp(x): Exponential function [Example: exp(1), exp(2)]
- log(x, base=exp(1)), log10(), log2(): Natural, base 10, and base 2 logarithms [Example: log(2), log10(100), log2(16)}
- sqrt(x): Square root [Example: sqrt(2)]
- sin(x), cos(x), tan(x), etc.: Trigonometric functions (see help(sin) for more) [Example: sin(pi)]
- abs(x): Absolute value [Example: abs(-3)]
- factorial(x): Factorial [Example: factorial(5)]
- choose(n, k): Binomial coefficient [Example: choose(5, 3)]
Note: Significant digits
- R prints 7 signifcant digits by default
- 'print()' function can be used to change sig fig
```{mathematic functions and sig figs}
# square root function
>  sqrt(3.5)

# squart root function with 10 sig fig
> print(sqrt(3.5), digits = 10)
```
Returned values can ether be saved as variable or passed directly to another function as an argument 
```{passing sqrt function into round function as  argument}
# passing sqrt function returned value into the round function
> round(sqrt(3.5))
[1] 2

# round to a specific decimal place by adding digit argument
> round(sqrt(3.5), digits=3)
[1] 1.871
> round(sqrt(3.5), 3)
[1] 1.871
```
## Variables and assignment
variable - a character or string that holds a value, assigned using '<-' assignment operator (Note: '=' can also be used to assign value to variable)
```{Assign value to variable x}
# assign value to variable x
> x <- 3.1

# retrieve value by entering variable into console
> x
3.1

# variables can be used in mathematics  expressions 
> (x + 2)/2
[1] 2.55
> exp(x)
[1] 22.1979513
> y <- exp(2*x) - 1
> y
[1] 491.749041
```
Values set to variables are assigned to an *environment* know the *global environment*
'ls()' function prints all asigned variable in global environment
```{print variables in environment}
> ls()
[1] "x"
```
# Vectors, vectorization and indexing
- *vector* - a container of contiguous data
  - vector of length 1 holds a single value
  - 'length()' function returns the length of a vector
```{vector length}
> length(3.1)
```
  - create longer vectors with 'c()' function
```{creating vector and storing in variable}
> x <- c(56, 95.3, 0.4)
> x
[1] 56.0 95.3  0.4
> y <- c(3.2, 1.1, 0.2)
> y
[1] 3.2 1.1 0.2
```
  - vectors can contain both variable-value pairs
```{vectors with values and variable  pairs}
> b <- c(a=3.4, b=5.4, c=0.4)
> b
  a   b   c
3.4 5.4 0.4

# variables can be called from vector
> names(b)
[1] "a" "b" "c"


```
- *vectorization* - process allowing for loop over vectors elementwise without needing to write an explicit loop
```{Vectorization and variable operations}
# store vector in x
 > x <- c(56, 95.3, 0.4)

# store vector in  y
> y <- c(3.2, 1.1, 0.2)

# Operations based on  values in corresponding vector positions in x and y
> x + y
[1] 59.2 96.4  0.6
> x - y
[1] 52.8 94.2  0.2
> x/y
[1] 17.50000 86.63636  2.00000

```
integer sequences function 'seq()' prints sequence between arguments
```{Seq function}
> seq(3, 5)
[1] 3 4 5
> 1:5
[1] 1 2 3 4 5
```
- *recycling* - vector operations performed when one vector is shorter than  the other, shorter vector applied  
-  allows for the addition  of a single value  to all elements in a  longer vector
```{Recycling vector operations}
> x
[1] 56.0 95.3  0.4
> x - 3
[1] 53.0 92.3 -2.6
> x / 2
[1] 28.00 47.65  0.20
> sqrt(x)
[1] 7.4833148 9.7621719 0.6324555
> round(sqrt(x), 3)
[1] 7.483 9.762 0.632
> log(x)/2 + 1 # note how we can combined vectorized operations
[1] 3.0126758 3.2785149 0.5418546
```
- recycling requires smaller group to be a multiple of the larget one
```{recylcing vectors with non-multiple vectors}
> c(1, 2) + c(0, 0, 0, 0) 
[1] 1 2 1 2
> c(1, 2) + c(0, 0, 0) 
[1] 1 2 1
Warning message:
In c(1, 2) + c(0, 0, 0) :
  longer object length is not a multiple of shorter object length
```
- *indexing* - accessing specific elements of a vector; an index is the integer that specifies  which element in the vector to retrieve
```{retrieving indexed vector elements}
# retrieving indexes from a vector
> x <- c(56, 95.3, 0.4)
> x[2] 
[1] 95.3
> x[1]
[1] 56
> x[4] 
[1] NA

# resigning an index in a vector
> x[3] <- 0.5 
> x
[1] 56.0 95.3  0.5

# names can be reset in place
> names(b) <- c("x", "y", "z")
> b
  x   y   z
3.4 5.4 0.4

# value-variable pairs can be retrieve from index
> b['x']
  x
3.4
> b['z']
  z
0.4
```
Vectors can be written to include certain values only or to exclude certain values
```{including or excluding values in a vector}
# extract indices 3-5 from vector z
> z <- c(3.4, 2.2, 0.4, -0.4, 1.2)
> z[3:5] # extract third, fourth, and fifth elements
[1]  0.4 -0.4  1.2

# exclude indices 4 and 5 from vector z
> z[c(-4, -5)]   # exclude fourth and fifth elements
[1] 3.4 2.2 0.4
```
Vectors can be reorganized based on indice number
# reorder indices in vector z
> z[c(3, 2, 4, 5, 1)]
[1]  0.4  2.2 -0.4  1.2  3.4


# reverse order in vector z
> z[5:1]
[1]  1.2 -0.4  0.4  2.2  3.4

# reordering using 'order()' function
> order(z)
[1] 4 3 5 2 1
> z[order(z)]
[1] -0.4  0.4  1.2  2.2  3.4
> order(z, decreasing=TRUE)
[1] 1 2 5 3 4
> z[order(z, decreasing=TRUE)]
[1]  3.4  2.2  1.2  0.4 -0.4
```

Comparison operators ('==', '!=', '<', '<=', '>', '>=') can be used to build *logical vectors*

R's comparison and logical operators
- >: Greater than
- <: Less than
- >=: Greater than or equal to
- <=: Less than or equal to
- ==: Equal to
- !: Not equal to
- &: Elementwise logical AND
- |: Elementwise logical OR
- !: Elementwise logical NOT
- &&: Logical AND (first element only, for if statements)
- ||: Logical OR (first element only, for if statements)

```{building logical vectors}
# building logical vectors based on comparison operators
> v <- c(2.3, 6, -3, 3.8, 2, -1.1)
> v == 6
[1] FALSE  TRUE FALSE FALSE FALSE FALSE
> v <= -3
[1] FALSE FALSE  TRUE FALSE FALSE FALSE
> abs(v) > 5
[1] FALSE  TRUE FALSE FALSE FALSE FALSE

# creating subsets or larger vector based on comparison operators
> v[v > 2]
[1] 2.3 6.0 3.8
```

## Vector types
Vectors in R must contain elements of the same type.  Supported data types:
- Numeric (double vectors): includes real numbhers
- Integers: whole numbers, positive or negative
- Characters: strings in quotations
- Logical: TRUE or FALSE
- Complex: complex (imaginary) numbers
- Raw: encodes raw bytes

R will coerce values based on coercion rules to ensure all elements in a vector are the same type
```{check the type of any object}
> q <- c(2, 3.5, -1.1, 3.8)
> typeof(q)
[1] "double”
```
- logical values coerced into 0 or 1
```{logical to numeric coercion}
> c(4.3, TRUE, 2.1, FALSE)
[1] 4.3 1.0 2.1 0.0
```
- interger values coerced into numeric
```{interger to numeric coercion}
# 'L' tag specifies a nmber as an integer
> c(-9L, 3.4, 1.2, 3L)
[1] -9.0  3.4  1.2  3.0
```
- if any value is a string, all values converted to string
```{string coercion}
> c(43L, TRUE, 3.2413341, "a string")
[1] "43"        "TRUE"      "3.2413341" "a string"
```

## Factors and classes in R

### Factors
factor - a kind of vector storing categorical variables (treatment, strand, chromosome etc)

Vectors can be convered to using 'factor()' function
- 'factor()' function also outputs "levels" which are the total values for the categorical variable
```{converting vectors to factors}
> chr_hits <- c("chr2", "chr2", "chr3", "chrX", "chr2", "chr3", "chr3")
> hits <- factor(chr_hits)
# "printing" a factor gives the vector values and the total levels
> hits
[1] chr2 chr2 chr3 chrX chr2 chr3 chr3
Levels: chr2 chr3 chrX

# 'levels()' function gives just the levels for a factor
> levels(hits)
[1] "chr2" "chr3" "chrX"
```

All possible levels can be specified (in situations where some values may not be present but you want them to be saved as possible values)
``` {defining levels}
> hits <- factor(chr_hits, levels=c("chrX", "chrY", "chr2", "chr3", "chr4"))
> hits
[1] chr2 chr2 chr3 chrX chr2 chr3 chr3
Levels: chrX chrY chr2 chr3 chr4
```
Existing levels in a factor can be renamed with 'levels()' 
```{renaming levels using 'list' function}
# 'hits' is already defined as a factor, we want to reset the possible levels using 'list'
> levels(hits) <- list(chrX="chrX", chrY="chrY", chr2="chr2",
                       chr3="chr3", chr4="chr4")
> hits
[1] chr2 chr2 chr3 chrX chr2 chr3 chr3
Levels: chrX chrY chr2 chr3 chr4
```

### Classes
class - endows an objecct with higher-level proterties that can affect how functions treat that object in R
- Factors are simply integers assigned a level variable name
```{assessing factor as integer values}
> typeof(hits)
[1] "integer"
> as.integer(hits)
[1] 3 3 4 1 3 4 4
```

Generic (*polymorphic*) functions  work on objects of all classes
- 'table()' is a generic function.  Total number of each level can be determined using 'table()' function
```{counting level in a factor}
> table(hits)
hits
chrX chrY chr2 chr3 chr4
   1    0    3    3    0
```
- 'summary()' is a genreic function that summarizes an object (i.e. vector) but its output depends on the class of values in the object
```{summary function output for numeric vs factor values}
> nums <- c(0.97, -0.7, 0.44, 0.25, -1.38, 0.08)
# summary function on "numeric" class data
> summary(nums)
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
-1.38000 -0.50500  0.16500 -0.05667  0.39250  0.97000
# summary function on "factor" class data
> summary(hits)
chrX chrY chr2 chr3 chr4
   1    0    3    3    0
```

# Working with and visualizing data in R
[Tutorial using data from "The Influence of Recombination on Human Genetic Diversity]<http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020148>.

## Loading Data into R
Note: in some cases, you may need to use unix tools to reformat data into tab-delimited format, use 'hexdump' to remove non-ASCII characters in files or coerce columns/remove improper values with R

*working directories* - the current working folder/directory in R; R defaults to home directory. 
- 'getwd()' function shows working directory
```{get working directory in r}
> getwd()
```
- 'setwd()' function changes directory 
```{change working directory in r}
> setwd ("~/bds-files-master/chapter-08-r")
```
Before load data into R, in the terminal use 'head' or 'cat' to review file structure (comments present, column delimits etc.)

r command for loading tables is 'read.table()'
- Note: 'read.table()' functions reformat some headers.  
	- 'spaces' are replaced with '
	- '%' are chanted to 'X'
- 'read.csv()' and 'read.delim()' are version of 'read.table()'
- large strings representing file names can be stored as variables in r to avoid repeat typing
```{loading dataset and storing as variable}
> d <- read.csv("Dataset_S1.txt")
```
- 'read.table()' and its other forms have a 'header' argument.  If a table starts with the names of the column the header will be 'TRUE'.  If the file does not contain headers, they van be assigned usig the 'col.names' argument (creads a vector wit hteh column names)
```{assigning column headers if not present in file}
> bd <- read.delim("noheader.bed", header=FALSE,
                   col.names=c("chrom", "start", "end"))
```
- by default, 'R encodes categorical 'read.delim()' and 'read.csv()' coerce column strings to a factor.  If you require to be a character vector instead set the argument 'stringsAsFactors=FALSE'

Table 8-4. Commonly used read.csv() and read.delim() arguments
Argument	Description	Additional comments
header
A TRUE/FALSE value indicating whether the first row contains column names rather than data
sep
A character value indicating what delimits columns; using the empty string "" treats all whitespace as a separator
stringsAsFactors
Setting this argument as FALSE prevents R from coercing character vectors to factors for all columns; see argument asis in help(read.delim) to prevent coercion to factor on specific columns
This is an important argument to be aware of, because R’s default behavior of coercing character vector columns to factors is a common stumbling block for beginners.
col.names
A character vector to be used as column names

row.names
A vector to use for row names, or a single integer or column name indicating which column to use as row names
na.strings
A character vector of possible missing values to convert to NA
Files using inconsistent missing values (e.g., a mix of “NA,” “Na,” “na”) can be corrected using na.strings=c("NA", "Na", "na").
colClasses
A character vector of column classes; "NULL" indicates a column should be skipped and NA indicates R should infer the type
colClasses can drastically decrease the time it takes to load a file into R, and is a useful argument when working with large bioinformatics files.
comment.char
Lines beginning with this argument are ignored; the empty string (i.e., "") disables this feature
This argument is useful in ignoring metadata lines in bioinformatics files.

## Long vs Wide Data
*wide data* - each variable has its own column, each row is for a new/single sample (normally how humans record data)
*long data* - one column is used to store what type of variable was measured and another is used to store the measurement
'reshape2' package provides functions to reshpate data
- 'melt()' function - turns wide into long data
- 'cast()' - turns long into wide data

## Exploring and transforming dataframes
*dataframe* - an R element consisting of columns (representing variables in your dataset) and rows (representiong observations); designed to hold columns of heterogeneous types of vectors

dataframe columns are vectors with each element being of the same type; different columns can be of differnt types 

'head()' function loads adn previews a dataframe
- defaults to showing 6 lines, can be changed with the 'n=<number>' argument
```{previewing with head function}
> head(d, n=3)
```

Dimensions of a dataframe can be evaluated using:
- 'nrow()' - number of rows
- 'ncol()' - number of columns
- 'dim()' - number of rows and columns 
- 'colnames()' - prints the column headers
- 'rownames()' - pritns the row headers
- converted column headers can be renamed to be more descriptive
```{renaming headers using column name function}
# specify the column to change by putting its position in '[]'
> colnames(d)[12] # original name
[1] "X.GC"
> colnames(d)[12] <- "percent.GC"
> colnames(d)[12] # after change
[1] "percent.GC"
```
- row names can be renamed as well (rows must have unique names)
- '$' operator can specify the column name and print that column as a vector.  This can be used to perform statistics on that column values
```{dollar sign operator to print column}
> d$depth
  [1]  3.41  6.68  9.06 10.26  8.06  7.05 [...]
# using column print to calculate average
“> mean(d$depth)
[1] 8.183938
> summary(d$depth)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  1.000   6.970   8.170   8.184   9.400  21.910
```
- data frame indexing finds data using two indexes. General syntax:  '<dataframe_variable>[row, col]'
	- Omitting the 'row' index returns all rows for the specified columns
```{return all rows in columns 1 and 2}
# retrieving two columns using column index
> d[ , 1:2]
  start   end
1 55001 56000
2 56001 57000
3 57001 58000
[...]
# retrieving two columns using a vector containing the column headers
> d[, c("start", "end")]
  start   end
1 55001 56000
2 56001 57000
3 57001 58000
[...]
# retrieve the first row
> d[1, ]
  start   end total.SNPs total.Bases depth unique.SNPs dhSNPs reference.Bases
1 55001 56000          0        1894  3.41           0      0             556
  Theta Pi Heterozygosity percent.GC Recombination  Divergence Constraint SNPs
1     0  0              0    54.8096   0.009601574 0.003006012          0    0
# retrieving a single cell
> d[2,3]
[1] 5
```
	- retrieiving a *single* column from a dataframe returns a *vector* by default.  To preserve dataframe class, use 'drop' option
```{retrieving single column and retaining dataframe class}
> d[, "start", drop=FALSE]
  start
1 55001
2 56001
3 57001
[...]
```

Columns can be added to dataframe
```{adding column to dataframe}
# adding column to represent a window in the centromere region
# dollar operator used to specify a column, for centromire window.  Ampersand is the vector logical operator for AND
> d$cent <- d$start >= 25800000 & d$end <= 29700000
# summarize information with 'table' function
> table(d$cent)
FALSE  TRUE
58455   685
# operations can be performed and stored in new columns
> d$diversity <- d$Pi / (10*1000)  # rescale, removing 10x and making per bp
> summary(d$diversity )
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
0.0000000 0.0005577 0.0010420 0.0012390 0.0016880 0.0265300
```

## Exploring Data through slicing and dicing: subsetting dataframes
*Dataframe subsetting* - specifying a portion of a dataget frame using R comparisons and logical operators

Exploring skewed data is a use of subsetting
```{exploring skewed data using subsetting}
# data thats skewed right and subset for values with 55 or moer SNPs
> summary(d$total.SNPs)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  0.000   3.000   7.000   8.906  12.000  93.000
# creating a logical vector for values in our dersired rand
> d$total.SNPs >= 85
[1] FALSE FALSE FALSE FALSE FALSE FALSE [...]
# creating a subset extracting rows from the data frame and saving in new dataframe
> d[d$total.SNPs >= 85, ]
         start      end total.SNPs total.Bases depth unique.SNPs dhSNPs
2567   2621001  2622000         93       11337 11.34          13     10
12968 13023001 13024000         88       11784 11.78          11      1
43165 47356001 47357000         87       12505 12.50           9      7
      reference.Bases  Theta     Pi Heterozygosity percent.GC Recombination
2567             1000 43.420 50.926         81.589    43.9439   0.000706536
12968            1000 33.413 19.030         74.838    28.8288   0.000082600
43165            1000 29.621 27.108         69.573    46.7467   0.000500577
      Divergence Constraint SNPs  cent
2567  0.01701702          0    1 FALSE
12968 0.01401401          0    1 FALSE
43165 0.02002002          0    7 FALSE
```

subsetting can increase in complexity by chaining comparision operators in queries
```{compound variables for subsetting queries}
> d[d$Pi > 16 & d$percent.GC > 80, ]
         start      end total.SNPs total.Bases depth unique.SNPs dhSNPs
58550 63097001 63098000          5         947  2.39           2      1
58641 63188001 63189000          2        1623  3.21           2      0
58642 63189001 63190000          5        1395  1.89           3      2
      reference.Bases  Theta     Pi Heterozygosity percent.GC Recombination
58550             397 37.544 41.172         52.784    82.0821   0.000781326
58641             506 16.436 16.436         12.327    82.3824   0.000347382
58642             738 35.052 41.099         35.842    80.5806   0.000347382
      Divergence Constraint SNPs  cent
58550 0.03826531        226    1 FALSE
58641 0.01678657        148    0 FALSE
58642 0.01793722          0    0 FALSE
```

complex queries using logic operators and retrieiving only specific columns
```{complex subsetting returning specific columns}
> d[d$Pi > 16 & d$percent.GC > 80, c("start", "end", "depth", "Pi")]
         start      end depth     Pi
58550 63097001 63098000  2.39 41.172
58641 63188001 63189000  3.21 16.436
58642 63189001 63190000  1.89 41.099
# reordering the select columns
> d[d$Pi > 16 & d$percent.GC > 80, c("start", "end", "Pi", "depth")]
         start      end     Pi depth
58550 63097001 63098000 41.172  2.39
58641 63188001 63189000 16.436  3.21
58642 63189001 63190000 41.099  1.89
# selecting only a vector information from the dataframe matching our logic statement
> d$percent.GC[d$Pi > 16]
[1] 39.1391 38.0380 36.8368 36.7367 43.0430 41.1411 [...]

Subsetting columns to summarize differnt conditions
```{determing if depth varies based on GC content}
> summary(d$depth[d$percent.GC >= 80])
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   1.05    1.89    2.14    2.24    2.78    3.37
> summary(d$depth[d$percent.GC < 80])
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  1.000   6.970   8.170   8.185   9.400  21.910
```

the negation ('!') operator can be used to subset data that is already logical varibles 
```{using negation operator to subset existing logical data}
> summary(d$Pi[d$cent])
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   0.00    7.95   16.08   20.41   27.36  194.40
> summary(d$Pi[!d$cent])
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  0.000   5.557  10.370  12.290  16.790 265.300
```

'which()' function returnes all TRUE values from a vector
```{subsetting based on TRUE using which function}
> d$Pi > 3
[1] FALSE  TRUE FALSE  TRUE  TRUE  TRUE [...]
> which(d$Pi > 3)
[1]  2  4  5  6  7 10 [...]
# using which to select first 4 true values in a set
> which(d$Pi > 10)[1:4]
[1]  2 16 21 23

'which.min()' and 'which.max()' functions return the first min/max elements of a vector
> d[which.min(d$total.Bases),]
         start      end total.SNPs total.Bases depth [...]
25689 25785001 25786000          0         110 1.24  [...]

> d[which.max(d$depth),]
       start     end total.SNPs total.Bases depth [...]
8718 8773001 8774000         58       21914 21.91 [...]
```

'subset()' function takes 2-3 arguments: 'subset(<dataframe>,<row_inclusion_conditions>,<column_inclusion_conditions)'
```{subset with 3 arguments}
> subset(d, Pi > 16 & percent.GC > 80,
    c(start, end, Pi, percent.GC, depth))
         start      end     Pi percent.GC depth
58550 63097001 63098000 41.172    82.0821  2.39
58641 63188001 63189000 16.436    82.3824  3.21
58642 63189001 63190000 41.099    80.5806  1.89
```

## Exploring data visually with ggplot2 part I: scatterplots and densities 
Install package
```{installing and loading ggplot2 package}
> install.packages("ggplot2")
> library(ggplot2)
```

'ggplot2' plots build grammaitacally; layers are added to a plot that map the aethetic properties of geometric objects to data.  Layers can:
- apply statistical transformatoins
- change scales, axes and colors

'ggplot2' function has two components: 'ggplot2(<dataframe>)+ geom_<subfunction>'
- 'ggplot2(<dataframe>)': function takes the data (dataframes only) to plot as input 
- 'geom_<subfunction>()': a geometric object "layer"
	- 'geom_point()' creates scatterplots
- 'aes()': funciton that maps th aesthetic attributes associated with graphing data (x and y positions, colors, shapes, sizes etc)

The example data is "windows based", requiring positioning information and midpoint between windows
```{adding position column for window placement}
# setting the values in a "position" column to be the X values and the "diversity" column to be the Y values
> d$position <- (d$end + d$start) / 2
> ggplot(d) + geom_point(aes(x=position, y=diversity))
# adding a column value to color
> ggplot(d) + geom_point(aes(x=position, y=diversity, color=cent))
```

*overplotting* - data oversaturating a plot so as to obscure the infromation of other data points.  Methods for visualizing overplotting:
- changing datapoint transparency (alpha) level so that only non overlapping poitns are dark
```{fixing overplotting by datapoint transparency}
# setting alpha transparency to all datapoints
> ggplot(d) + geom_point(aes(x=position, y=diversity), alpha=0.01)
# setting alpha transparency to all points but over lapping parts of points add their opacities
> ggplot(d) + geom_point(aes(x=position, y=diversity,alpha=0.01))
```
- density histograms to visualize occurances 
```{histogram plots to show overplotting as density}
> ggplot(d) + geom_density(aes(x=diversity), fill="black")
# coloring can be set to another parameter 
```

## Exploring data visually with ggplot2 Part II: smoothing
'geom_smooth()' sub function adds a smoothing line to the plots; requires x and y aesthetics
```{geo_smooth layer on scatterplot}
> ggplot(d, aes(x=depth, y=total.SNPs)) + geom_point() + geom_smooth()
```
- defaults to generalized additive models (GAM) for dataset smoothing with more than 1000 rows
- confidence intervals are added around the smoothing line
- checking possible confounding variables by ploting other combination of columns
```{checking GC content on data smoothing}
> ggplot(d, aes(x=percent.GC, y=depth)) + geom_point() + geom_smooth()
```

## Binning data with cut() and bar plots with ggplot2
*binning* (or discretization) - taking continuous numerica values and placing them into discrete ranges(bins)
- discrete binning facilitates *conditioning* on a variable
- 'cut(<dataframe>, <number_of_bins>)' function - takes the specified dataframe and divides it into the specified number of (equally sized) bins rteturning a 'factor'
```{binning data example}
> d$GC.binned <- cut(d$percent.GC, 5)
> d$GC.binned
[1] (51.6,68.5] (34.7,51.6] (34.7,51.6] (34.7,51.6] (34.7,51.6]
[...]
Levels: (0.716,17.7] (17.7,34.7] (34.7,51.6] (51.6,68.5] (68.5,85.6]
# detemring how many values fall into specified bins
> table(d$GC.binned)
(0.716,17.7]  (17.7,34.7]  (34.7,51.6]  (51.6,68.5]  (68.5,85.6]
           6         4976        45784         8122          252
```
- 'breaks' - element that can be added to a vector to set prespecified ranges (bins)
```{setting 'breaks' with cut function}
> cut(d$percent.GC, c(0, 25, 50, 75, 100))
[1] (50,75] (25,50] (25,50] (25,50] (25,50] (25,50]
[...]
Levels: (0,25] (25,50] (50,75] (75,100]
```
	- Note: 'breaks' assign NA values to data outside of specified bins.  Check for lost data using 'any(is.na(cut(<dataframe>, breaks)))''

Binned data best visualized using bar plots with bar plots and/or density plots
- 'ggplot2''s 'geom_bar()' command (the aesthetic is set to the binned variable)
```{creating bar plot of binned data}
> ggplot(d) + geom_bar(aes(x=GC.binned))
```
- density plots can be used to futher view differnces in binned data
```{viewing binned data in density plot}
> ggplot(d) + geom_density(aes(x=depth, linetype=GC.binned), alpha=0.5)
```

## Merging and combining data: matching vectors and merging dataframes 
'%in%' operator - matches two vectors (x and y) checking whetehr some of a vectors values are in another vector, returning a logical vector
```{checking if any elements in x are in y}
> c(3, 4, -1) %in% c(1, 3, 4, 8)
[1]  TRUE  TRUE FALSE
```
- '%in%' often used to select rows from a dataframe by specifying the leveles a factor column can take 
```{creating 'reps' dataset}
> reps <- read.delim("chrX_rmsk.txt.gz", header=TRUE)
> head(reps, 3)
  bin swScore milliDiv milliDel milliIns genoName genoStart genoEnd   genoLeft
1 585     342        0        0        0     chrX         0      38 -154824226
2 585     392      109        0        0     chrX        41     105 -154824159
3 585     302      240       31       20     chrX       105     203 -154824061
  strand   repName      repClass     repFamily repStart repEnd repLeft id
1      + (CCCTAA)n Simple_repeat Simple_repeat        3     40       0  1
2      +    LTR12C           LTR          ERV1     1090   1153    -425  2
3      +     LTR30           LTR          ERV1      544    642     -80  3
# CREATE A vector of the terms we want to find in common
> common_repclass <- c("SINE", "LINE", "LTR", "DNA", "Simple_repeat")
> reps[reps$repClass %in% common_repclass, ]
  bin swScore milliDiv milliDel milliIns genoName genoStart genoEnd   genoLeft
1 585     342        0        0        0     chrX         0      38 -154824226
2 585     392      109        0        0     chrX        41     105 -154824159
3 585     302      240       31       20     chrX       105     203 -154824061
[...]
  strand   repName      repClass     repFamily repStart repEnd repLeft id
1      + (CCCTAA)n Simple_repeat Simple_repeat        3     40       0  1
2      +    LTR12C           LTR          ERV1     1090   1153    -425  2
3      +     LTR30           LTR          ERV1      544    642     -80  3
[...]
- alterantively, instead of setting a vector of the common repclasses, we can pull out the existing repclasses in a file progrtammatically
```{pulling out existing repclasses}
# pull out the 5 most commonly repated classes
> sort(table(reps$repClass), decreasing=TRUE)[1:5]
         SINE          LINE           LTR           DNA Simple_repeat
        45709         30965         14854         11347          9163
# taking these top 5 and creating a vector  
> top5_repclass <- names(sort(table(reps$repClass), decreasing=TRUE)[1:5])
> top5_repclass
[1] "LINE"          "SINE"          "LTR"           "Simple_repeat"
[5] "DNA"
```

'match()' is the larger function that the '%in%' operator comes from
```{the 'match' function}
> match(c("A", "C", "E", "A"), c("A", "B", "A", "E"))
[1]  1 NA  4  1
```
- 'match(x,y)' function returnes the first occurence of each of x's values in y, returning an NA if no match is found
- 'x' is what you are search for, 'y' is where you are seraching
- 'match()' returns the POSITION of the first OCCURANCE, not whether or not its present (which is what %in% does) nor does it return the total number of times a value is present in the y data

Since 'match()' returns *where* a particular value is found, the functions output can be used to join two dataframes together by shared columns
```{comining data from two datasets}
# load the two datasets to be compared
> mtfs <- read.delim("motif_recombrates.txt", header=TRUE)
> head(mtfs, 3)
   chr motif_start motif_end    dist recomb_start recomb_end  recom
1 chrX    35471312  35471325 39323.0     35430651   35433340 0.0015
2 chrX    35471312  35471325 36977.0     35433339   35435344 0.0015
3 chrX    35471312  35471325 34797.5     35435343   35437699 0.0015
          motif
1 CCTCCCTGACCAC
2 CCTCCCTGACCAC
3 CCTCCCTGACCAC
> rpts <- read.delim("motif_repeats.txt", header=TRUE)
> head(rpts, 3)
   chr     start       end name motif_start
1 chrX  63005829  63006173   L2    63005830
2 chrX  67746983  67747478   L2    67747232
3 chrX 118646988 118647529   L2   118647199
```
- Step 1: when two or moer columns are common between two datsets, concatenate the columns into a single *key* string
```{merging data set 1 - create concatenated key of common columns}
> mtfs$pos <- paste(mtfs$chr, mtfs$motif_start, sep="-")
> rpts$pos <- paste(rpts$chr, rpts$motif_start, sep="-")
> head(mtfs, 2) # results
   chr motif_start motif_end  dist recomb_start recomb_end  recom         motif
1 chrX    35471312  35471325 39323     35430651   35433340 0.0015 CCTCCCTGACCAC
2 chrX    35471312  35471325 36977     35433339   35435344 0.0015 CCTCCCTGACCAC
            pos
1 chrX-35471312
2 chrX-35471312
> head(rpts, 2)
   chr    start      end name motif_start           pos
1 chrX 63005829 63006173   L2    63005830 chrX-63005830
2 chrX 67746983 67747478   L2    67747232 chrX-67747232
```
- Step 2: validate the *key* string overlap in the way expected BEFORE merging
```{using table and %in% operator to see how many elements in dataframe x have corresponding entries in dataframe y}
> table(mtfs$pos %in% rpts$pos)
# check to see if our keys match between datasets 
FALSE  TRUE
10832  9218
# find where each match is
i <- match(mtfs$pos, rpts$pos)
# verify that the number of matches (TRUE and FALSE values) is the same as calculatd in step 1
> table(is.na(i))
FALSE  TRUE
 9218 10832
```
- Step 3: using the index vector 'i', select out the matching elemlents from rpts and merge into mtfs
> mtfs$repeat_name <- rpts$name[i]
# if you skip directly to this step without above checks
> mtfs$repeat_name <- rpts$name[match(mtfs$pos, rpts$pos)]
```
- Step 4: Validate the accuracy of merged data
```{using head to check the newly added column, verify overlap with UCSC Genome Browser}
> head(mtfs[!is.na(mtfs$repeat_name), ], 3)
     chr motif_start motif_end    dist recomb_start recomb_end  recom
102 chrX    63005830  63005843 37772.0     62965644   62970485 1.4664
103 chrX    63005830  63005843 34673.0     62970484   62971843 0.0448
104 chrX    63005830  63005843 30084.5     62971842   62979662 0.0448
            motif           pos repeat_name
102 CCTCCCTGACCAC chrX-63005830          L2
103 CCTCCCTGACCAC chrX-63005830          L2
104 CCTCCCTGACCAC chrX-63005830          L2
# if data verifed, option to remove NAs
> mtfs_inner <- mtfs[!is.na(mtfs $repeat_name), ]
> nrow(mtfs_inner)
[1] 9218
```

'merge()' function can directly merge two datasets (x and y) by tcolumns speicfied with 
'by.x="<column>"' and 'by.y="<column>"''
```{merge function}
> recm <- merge(mtfs, rpts, by.x="pos", by.y="pos")
> head(recm, 2)
             pos chr.x motif_start.x motif_end    dist recomb_start recomb_end
1 chr1-101890123  chr1     101890123 101890136 34154.0    101855215  101856736
2 chr1-101890123  chr1     101890123 101890136 35717.5    101853608  101855216
   recom         motif repeat_name chr.y     start       end  name
1 0.0700 CCTCCCTAGCCAC       THE1B  chr1 101890032 101890381 THE1B
2 0.0722 CCTCCCTAGCCAC       THE1B  chr1 101890032 101890381 THE1B
  motif_start.y
1     101890123
2     101890123
> nrow(recm)
[1] 9218
```

## Using ggplot2 facets
*facets* - allow visualization of grouped data by creating a series of separate adjacent plots for each group
```{}
# creating plot
> p <- ggplot(mtfs, aes(x=dist, y=recom)) + geom_point(size=1)

> p <- p + geom_smooth(method="loess", se=FALSE, span=1/10)
> print(p)
# this data can be displayed based on a third factor motif
> ggplot(mtfs, aes(x=dist, y=recom)) + geom_point(size=1) +
    geom_smooth(aes(color=motif), method="loess", se=FALSE, span=1/10)
```
	
ggplot2 has two facet methods: 
- facet_wrap(): takes a column factor and creates a panel for each level and wraps around horizontally
```{wrap facet}
	# the motifs can be split into differnt graphs 
> p <- ggplot(mtfs, aes(x=dist, y=recom)) + geom_point(size=1, color="grey")
> p <- p + geom_smooth(method='loess', se=FALSE, span=1/10)
> p <- p + facet_wrap(~ motif)
> print(p)
```

- facet_grid(): allows user to specify which columns to use for vertical and horizontal facets
```{grid facet}
> p <- ggplot(mtfs, aes(x=dist, y=recom)) + geom_point(size=1, color="grey")
> p <- p + geom_smooth(method='loess', se=FALSE, span=1/16)
> p <- p + facet_grid(repeat_name ~ motif)
> print(p)
```

## More R data structures: lists
*lists* are data structure more flexible that vectors.  List can
- contain elements of different types (heterogeneous)
- elements can be *any* R object (vectors, other lists, environments, dataframes, matrices, functions etc)
- allows for storing data in "reclusive" manner (lists can contain other lists, vectors can not contain other vectors)

Dataframes are built on using R list

'is.list()' function can be used to determine

create lists with 'list()' function
```{creating lists}
> adh <- list(chr="2L", start=14615555L, end=14618902L, name="Adh")
> adh
$chr
[1] "2L"
$start
[1] 14615555
$end
[1] 14618902
$name
[1] "Adh"
```

Accessing information in a list has two operators:
- single brackets ([]): used to access a subset of multiple elements as a list
```{accessing elements from a list, returning a list}
> adh[1:2]
$chr
[1] "2L"
$start
[1] 14615555
	- single brackets always returns a list
```
- double bracket ([[]]): used toaccessing an element without a list
```{accessing single element from a list, from specified index entered}
> adh[[2]]
[1] 14615555
> adh[['start']]
[1] 14615555
```
	- Note: the syntactic shortcut for double brakets is the retrieve column command we've used previously ('<dataframe>$<column>:')

Items in a list can be renamed using '<-' operator
```{examples of renaming list elements}
> adh$id <- "FBgn0000055"
> adh$chr <- "chr2L"
> adh
$chr
[1] "chr2L"
$start
[1] 14615555
$end
[1] 14618902
$name
[1] "Adh"
$id
[1] "FBgn0000055"

# renaming using 'NULL removes the entry'
> adh$id <- NULL # remove the FlyBase ID
> adh
$chr
[1] "chr2L"
$start
[1] 14615555
$end
[1] 14618902
$name
[1] "Adh"
# List names can be changed using 'names()' or 'names() <-'
```

'str()' function used to investigfate the structure of an R element

## Writing and applying functions to lists with lapply() and sapply()
Methods similar to 'for' loops in other languages (although R does have its own verion of 'for' loops)

### Using lapply
list apply ('lapply') 
```{using lapply}
# defining data in a varianle 'll'
> ll <- list(a=rnorm(6, mean=1), b=rnorm(6, mean=4), c=rnorm(6, mean=6))
> ll
$a
[1]  2.2629543  0.6737666  2.3297993  2.2724293  1.4146414 -0.5399500
$b
[1] 3.071433 3.705280 3.994233 6.404653 4.763593 3.200991
$c
[1] 4.852343 5.710538 5.700785 5.588489 6.252223 5.108079
# detrmining the mean for a, b and c 
> lapply(ll, mean)
$a
[1] 0.5103648
$b
[1] 0.09681026
$c
[1] -0.2847329
```

### Writing functions
Function definiations have 3 elements:
- argument
- body
- return
- General syntax for R function
fun_name <- function(args) {
   # body, containing R expressions
   return(value)
}
```{writing functions in r}
# to ignore NA values when calulating mean
> lapply(ll, mean, na.rm=TRUE)
# to create a function that removes NA then performs means 
> meanRemoveNA <- function(x) mean(x, na.rm=TRUE)
> lapply(ll, meanRemoveNA)
$a
[1] 1.216768
$b
[1] 4.19003
$c
[1] 5.53541
# creating functions with warnings important when documenting custom work
meanRemoveNAVerbose <- function(x, warn=TRUE) {
  # A function that removes missing values when calculating the mean
  # and warns us about it.
    if (any(is.na(x)) && warn) {
      warning("removing some missing values!")
    }
  mean(x, na.rm=TRUE)
}
```

writing functions creates a global environment object, an anonymous function (a function without a name)

anonymous functions are useful for one-off solutions for a specific tasks 

### Digression: debugging R code (COME BACK TO LATER)
Method 1: Pausing execution at a 
"breaking point" using 'browser()' function
```{troubleshooting with browswer function}
foo <- function(x) {
  a <- 2
  browser()
  y <- x + a
  return(y)
}
# Load this function into R, and then run:
> foo(1)
Called from: foo(1)
Browse[1]>
# 'n' command executes the next line
# 'c' command continues running the code
# 'Q' command exits without continuing to run code
```
### More list apply functions: sapply() and mapply()
'sapply()' function: similar to lapply(), simplifies results into vector, array or matrix
```{sapply function}
> sapply(ll, function(x) mean(x, na.rm=TRUE))
       a        b        c
1.216768 4.190030 5.535410
```

'mapply()' function: multivariant version of sapply(), can take in and use multiple arugments 
```{mapply example}
> ind_1 <- list(loci_1=c("T", "T"), loci_2=c("T", "G"), loci_3=c("C", "G"))
> ind_2 <- list(loci_1=c("A", "A"), loci_2=c("G", "G"), loci_3=c("C", "G"))
> mapply(function(a, b) length(intersect(a, b)), ind_1, ind_2)
loci_1 loci_2 loci_3
     0      1      2
```

##	Workign with the split-apply-combine pattern 
**Split**
- split(<dataframe>,<grouping_factor>) command splits a dataframe or vector 
```{splitting function}
> d_split <- split(d$depth, d$GC.binned)
> str(d_split)
List of 5
 $ (0.716,17.7]: num [1:6] 4.57 1.12 6.95 2.66 3.69 3.87
 $ (17.7,34.7] : num [1:4976] 8 8.38 9.02 10.31 12.09 ...
 $ (34.7,51.6] : num [1:45784] 6.68 9.06 10.26 8.06 7.05 ...
 $ (51.6,68.5] : num [1:8122] 3.41 7 6.63 7.15 6.97 4.77 5.18 ...
 $ (68.5,85.6] : num [1:252] 8.04 1.96 3.71 1.97 4.82 4.22 3.76 ...
```
	- split function returns list with each element containing all observations for a parrticlar lvel of the factor used in grouping

**Apply**
Perform function to each group using 'lapply()' function
```{applying function}
> lapply(d_split, mean)
$`(0.716,17.7]`
[1] 3.81
$`(17.7,34.7]`
[1] 8.788244
$`(34.7,51.6]`
[1] 8.296699
$`(51.6,68.5]`
[1] 7.309941
$`(68.5,85.6]`
[1] 4.037698
```

**Combine**
Putting together the results of the above steps

routine method for combining a list of vectors is by binding each element toge3ther into a matrix or dataframe using column bind ('cbind()') or row bind ('rbind()')
```{column and row binding}
> dpth_summ <- lapply(split(d$depth, d$GC.binned), summary)
> dpth_summ
$`(0.716,17.7]`
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  1.120   2.918   3.780   3.810   4.395   6.950
$`(17.7,34.7]`
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  1.000   7.740   8.715   8.788   9.800  17.780
  [...]
> rbind(dpth_summ[[1]], dpth_summ[[2]])
     Min. 1st Qu. Median  Mean 3rd Qu.  Max.
[1,] 1.12   2.918  3.780 3.810   4.395  6.95
[2,] 1.00   7.740  8.715 8.788   9.800 17.78

> cbind(dpth_summ[[1]], dpth_summ[[2]])
         [,1]   [,2]
Min.    1.120  1.000
1st Qu. 2.918  7.740
Median  3.780  8.715
Mean    3.810  8.788
3rd Qu. 4.395  9.800
Max.    6.950 17.780
```
- Note: good for short lists

Longer lists to combine use 'do.call()' function
- takes a function and a list as arguments, runs the items in list as the functions argumnents
```{do.call and rbind to create result matrix}
> do.call(rbind, lapply(split(d$depth, d$GC.binned), summary))
             Min. 1st Qu. Median  Mean 3rd Qu.  Max.
(0.716,17.7] 1.12   2.918  3.780 3.810   4.395  6.95
(17.7,34.7]  1.00   7.740  8.715 8.788   9.800 17.78
(34.7,51.6]  1.00   7.100  8.260 8.297   9.470 21.91
(51.6,68.5]  1.00   6.030  7.250 7.310   8.540 21.18
(68.5,85.6]  1.00   2.730  3.960 4.038   5.152  9.71
```

'tapply()' and 'aggregate()' can be used to create per-group summaries
```{tapply and aggregate}
> tapply(d$depth, d$GC.binned, mean)
(0.716,17.7]  (17.7,34.7]  (34.7,51.6]  (51.6,68.5]  (68.5,85.6]
    3.810000     8.788244     8.296699     7.309941     4.037698
> aggregate(d$depth, list(gc=d$GC.binned), mean)
            gc        x
1 (0.716,17.7] 3.810000
2  (17.7,34.7] 8.788244
3  (34.7,51.6] 8.296699
4  (51.6,68.5] 7.309941
5  (68.5,85.6] 4.037698
```

## Exploring Dataframes with dplyr
dplyr consolidates and simplifies many common operations performed on dataframes

'dplyr' uses a dat class called 'tbl_df' (a wrapper for dataframes).  Uses 'tbld_df()' function
```{installing and converting dataframe to tbl_df}
# method from text
d_df<-tbl_df(d)

# tbl_df() was depricated, new version is as_tibble
d_df<-as_tibble(d)
```
dplyr is a Library with 5 basic functions: arrange(), filter(), mutate(), select(), and summarize()
- 'select()' function for selecting columns from dataframe 
(This is equivalent to 'd[, c("start", "end", "Pi", "Recombination", "depth")]')
```{dplyr select function}
> select(d_df, start, end, Pi, Recombination, depth)
Source: local data frame [59,140 x 5]

   start   end     Pi Recombination depth
1  55001 56000  0.000   0.009601574  3.41
2  56001 57000 10.354   0.009601574  6.68
3  57001 58000  1.986   0.009601574  9.06
[...]
# dplyr also recognizes consecutive columns 
select(d_df, start:total.Bases)
# dplyr can return all but selected commons whne using negative symbol operator
> select(d_df, -(start:cent))
Source: local data frame [59,140 x 3]

   position   GC.binned diversity
1   55500.5 (51.6,68.5] 0.0000000
2   56500.5 (34.7,51.6] 0.0010354
3   57500.5 (34.7,51.6] 0.0001986
[...]
```
- 'filter()' function used to subset dataframes (equivalent to 'd[d$Pi > 16 & d$percent.GC > 80, ]').  'filter()' supports multiple statments
```{dyplr filter function}
> filter(d_df, Pi > 16, percent.GC > 80)
Source: local data frame [3 x 20]
start      end total.SNPs total.Bases depth unique.SNPs dhSNPs
1 63097001 63098000          5         947  2.39           2      1
2 63188001 63189000          2        1623  3.21           2      0
3 63189001 63190000          5        1395  1.89           3      2
Variables not shown: reference.Bases (int), Theta (dbl), Pi (dbl),
  Heterozygosity (dbl), percent.GC (dbl), Recombination (dbl), Divergence
  (dbl), Constraint (int), SNPs (int), cent (lgl), position (dbl), GC.binned
  (fctr), diversity (dbl)
```
- 'arrange()' function sorts by columns (equivalent to 'd[order(d$percent.GC), ]')
```{dplyr arrange function}
> arrange(d_df, depth)
Source: local data frame [59,140 x 20]
      start      end total.SNPs total.Bases depth unique.SNPs dhSNPs
1   1234001  1235000          0         444     1           0      0
2   1584001  1585000          0         716     1           0      0
3   2799001  2800000          0         277     1           0      0
[...]
# sort desending order
> arrange(d_df, desc(total.SNPs), desc(depth))
Source: local data frame [59,140 x 20]
      start      end total.SNPs total.Bases depth unique.SNPs dhSNPs
1   2621001  2622000         93       11337 11.34          13     10
2  13023001 13024000         88       11784 11.78          11      1
3  47356001 47357000         87       12505 12.50           9      7
[...]
```
- 'mutate()' allows for adding new columns (equivalant to earlier example that rescaled version of the Pi column as d$diversity)
```{dplyr mutate function}
# remove the diversity column and replace it using the mutate function
> d_df <- select(d_df, -diversity) # remove our earlier diversity column
> d_df <- mutate(d_df, diversity = Pi/(10*1000))
> d_df
Source: local data frame [59,140 x 20]
   start   end total.SNPs total.Bases depth unique.SNPs dhSNPs reference.Bases
1  55001 56000          0        1894  3.41           0      0             556
2  56001 57000          5        6683  6.68           2      2            1000
3  57001 58000          1        9063  9.06           1      0            1000
[...]
..   ...   ...        ...         ...   ...         ...    ...             ...
Variables not shown: Theta (dbl), Pi (dbl), Heterozygosity (dbl), percent.GC
  (dbl), Recombination (dbl), Divergence (dbl), Constraint (int), SNPs (int),
  cent (lgl), position (dbl), GC.binned (fctr), diversity (dbl)
  
```

'dplyr' functions can be chained together for speed and processor efficency
- nested functions can be used to chain functions 
	- Example: 'filter(select(hs_df, seqname, start, end, strand), strand == "+"))'
- 'dplyr' also supports *pipes* ('%>%') using the 'magrittr' package
```{installing and running magrittr pipes}
# The easiest way to get magrittr is to install the whole tidyverse:
install.packages("tidyverse")
# Alternatively, install just magrittr:
install.packages("magrittr")
# Or the development version from GitHub:
# install.packages("devtools")
devtools::install_github("tidyverse/magrittr")
# using pipes for various dplyr functions
>  d_df %>% mutate(GC.scaled = scale(percent.GC)) %>%
            filter(GC.scaled > 4, depth > 4) %>%
            select(start, end, depth, GC.scaled, percent.GC) %>%
            arrange(desc(depth))
Source: local data frame [18 x 5]
      start      end depth GC.scaled percent.GC
1  62535001 62536000  7.66  4.040263    73.9740
2  63065001 63066000  6.20  4.229954    75.3754
3  62492001 62493000  5.25  4.243503    75.4755
[...]
```

'group_by()' dplyr function can be used to group data based on column levels
```{dplyr grouping by chromosomes}
> mtfs_df <- as_tibble(mtfs)
> mtfs_df %>% group_by(chr)
Source: local data frame [20,050 x 10]
Groups: chr
chr motif_start motif_end    dist recomb_start recomb_end  recom
1  chrX    35471312  35471325 39323.0     35430651   35433340 0.0015
2  chrX    35471312  35471325 36977.0     35433339   35435344 0.0015
[...]
```

'dplyr''s 'sumarize()' function (equivalent to 'tapply()' and 'aggregate()')
- function takes relevant columns with the supplied argument names
```{creating summaries of piped dplyr functions}
>  mtfs_df %>%
     group_by(chr) %>%
     summarize(max_recom = max(recom), mean_recom = mean(recom), num=n())
Source: local data frame [23 x 4]
     chr max_recom mean_recom  num
1   chr1   41.5648   2.217759 2095
2  chr10   42.4129   2.162635 1029
3  chr11   36.1703   2.774918  560
[...]
```
- common summary function supported in 'summarize()'
	- 'n()': returns number of obersations in each group
	- 'n_distinct()': returns unique numhner of obersations in each group
	- 'first()', 'last()
	', 'nth()
	': returns the stated observation (best used with data that hasbeen sorted with 'arrange()')
```{example summary wiht position data piped into a sumarizing function}
“>  mtfs_df %>%
     group_by(chr) %>%
     summarize(max_recom = max(recom), mean_recom = mean(recom), num=n()) %>%
     arrange(desc(max_recom))
Source: local data frame [23 x 4]
     chr max_recom mean_recom  num
1   chrX   74.0966   2.686840  693
2   chr8   62.6081   1.913325 1727
3   chr3   56.2775   1.889585 1409
4  chr16   54.9638   2.436250  535
[...]
```

# Working with strings
Note: r is not the best language to use when manipulating text.  Use r when:
- you've already imported data into R

All strings in r are character vectors (everthing within quotations counts as one element/ length=1)
- to return the number characters within a string:
```{characters within a string}
> nchar(c("AGCTAG", "ATA", "GATCTGAG", ""))
[1] 6 3 8 0
```

Searching 